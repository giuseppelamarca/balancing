{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giuseppe/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/giuseppe/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/giuseppe/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/giuseppe/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/giuseppe/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/giuseppe/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "#import numpy as np\n",
    "import pickle\n",
    "import rospy\n",
    "import sys\n",
    "from geometry_msgs.msg import Twist\n",
    "from sensor_msgs.msg import Imu, JointState\n",
    "from std_srvs.srv import Empty\n",
    "from nav_msgs.msg import Odometry\n",
    "from sensor_msgs.msg import LaserScan\n",
    "from std_msgs.msg import Float32, Float64, Int32\n",
    "import time\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from gazebo_msgs.msg import ModelStates\n",
    "from tensorboardX import SummaryWriter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"logdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheel_l = \"/cmd_vel\"#/self_balancing_ai/wheell_velocity_controller/command\"\n",
    "wheel_r = \"/cmd_vel\"#/self_balancing_ai/wheelr_velocity_controller/command\"\n",
    "real_robot = \"/real_robot_wheel\"\n",
    "\n",
    "\n",
    "class NN_ROBOT1:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.joint1 = rospy.Publisher(wheel_l,Twist,queue_size =1)\n",
    "        self.joint2 = rospy.Publisher(wheel_r,Twist,queue_size =1)      \n",
    "        self.real_robot = rospy.Publisher(real_robot,Int32,queue_size =1)\n",
    "        \n",
    "        self.joint1_msg = Twist()\n",
    "        self.joint2_msg = Twist() \n",
    "        self.real_robot_msg = Int32()\n",
    "        self.avanza = 0 \n",
    "        self.gira = 0\n",
    "        self.new_angle = False\n",
    "        \n",
    "        #'''\n",
    "        subscriber = rospy.Subscriber(\"/gazebo/model_states\",ModelStates,callback=self.gazebo_callback)\n",
    "        self.subscriber = rospy.Subscriber(\"/segway/joint_states\",JointState,callback=self.robot_configuration_callback)\n",
    "        self.subscriber = rospy.Subscriber(\"/imu\",Imu,callback=self.imu_callback)\n",
    "        self.teleop = rospy.Subscriber(\"/turtle1/cmd_vel\", Twist ,callback=self.teleop_callback)\n",
    "        self.subscriber = rospy.Subscriber(\"/segzay/laser/scan\", LaserScan, callback=self.laser_callback)\n",
    "        self.reset = rospy.ServiceProxy(\"/gazebo/reset_world\",Empty)\n",
    "        self.pause = rospy.ServiceProxy(\"/gazebo/pause_physics\",Empty)\n",
    "        self.unpause = rospy.ServiceProxy(\"/gazebo/unpause_physics\",Empty)\n",
    "        #'''\n",
    "        \n",
    "        #self.subscriber = rospy.Subscriber(\"/angle\",Float32,callback=self.mpu5060_callback)        \n",
    "        self.distance = 0\n",
    "        self.tilt = 0\n",
    "        self.postion = 0\n",
    "        self.q = []\n",
    "        self.roll = 0\n",
    "        self.pitch= 0 \n",
    "        self.yaw = 0 \n",
    "\n",
    "        self.robot_configuration = []\n",
    "\n",
    "    def laser_callback(self,data):\n",
    "        self.distance = data.ranges[0]\n",
    "        \n",
    "    def reset_pose(self):\n",
    "            Robot.joint1_msg.angular.z = 0\n",
    "            Robot.joint2_msg.angular.z = 0\n",
    "            \n",
    "            Robot.joint1.publish(Robot.joint1_msg)\n",
    "            Robot.joint2.publish(Robot.joint2_msg)           \n",
    "    \n",
    "    def teleop_callback(self,data):\n",
    "        self.avanza += data.linear.x* 0.01\n",
    "        self.gira = data.angular.z\n",
    "    def mpu5060_callback(self, date):\n",
    "        self.new_angle = True\n",
    "        self.roll = date.data * 0.01745329\n",
    "        \n",
    "    def imu_callback(self, date):\n",
    "        self.q = [\n",
    "            date.orientation.x,\n",
    "            date.orientation.y,\n",
    "            date.orientation.z,\n",
    "            date.orientation.w]\n",
    "        x, y, z, w = self.q\n",
    "        t0 = +2.0 * (w * x + y * z)\n",
    "        t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "        self.roll = math.atan2(t0, t1)\n",
    "        t2 = +2.0 * (w * y - z * x)  \n",
    "        t2 = +1.0 if t2 > +1.0 else t2\n",
    "        t2 = -1.0 if t2 < -1.0 else t2\n",
    "        self.pitch = math.asin(t2)\n",
    "        t3 = +2.0 * (w * z + x * y)\n",
    "        t4 = +1.0 - 2.0 * (y * y + z * z)\n",
    "        self.yaw = math.atan2(t3, t4)\n",
    "    \n",
    "    \n",
    "    def robot_configuration_callback(self, data):\n",
    "        self.robot_configuration = data.position\n",
    "        \n",
    "    def gazebo_callback(self, data):\n",
    "        self.position = data.pose[1].position.y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_var, action_std):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # action mean range -1 to 1\n",
    "        self.actor =  nn.Sequential(\n",
    "                nn.Linear(state_dim, n_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_var, n_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_var, action_dim),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_var, n_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_var, 1)\n",
    "                )\n",
    "        self.action_var = torch.full((action_dim,), action_std*action_std).to(device)\n",
    "        \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, state, memory):\n",
    "        action_mean = self.actor(state)\n",
    "        dist = MultivariateNormal(action_mean, torch.diag(self.action_var).to(device))\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob)\n",
    "        \n",
    "        return action.detach()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        action_mean = self.actor(state)\n",
    "        dist = MultivariateNormal(torch.squeeze(action_mean), torch.diag(self.action_var))\n",
    "        \n",
    "        action_logprobs = dist.log_prob(torch.squeeze(action))\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_value = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, action_std, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, n_latent_var, action_std).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(),\n",
    "                                              lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, n_latent_var, action_std).to(device)\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "    \n",
    "    def select_action(self, state, memory):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def update(self, memory):\n",
    "        print (\"Updating policy...\")\n",
    "        # Monte Carlo estimate of rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward in reversed(memory.rewards):\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).to(device).detach()\n",
    "        old_actions = torch.stack(memory.actions).to(device).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()\n",
    "\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "     \n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "    def load(self, path): \n",
    "        checkpoint = torch.load(path)\n",
    "        ppo.policy_old.load_state_dict(checkpoint['old_policy_state_dict'])\n",
    "        ppo.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "        ppo.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        ppo.policy_old.train()\n",
    "        ppo.policy.train()\n",
    "        \n",
    "    def save(self,path):\n",
    "        torch.save({\n",
    "            'old_policy_state_dict': self.policy_old.state_dict(),\n",
    "            'policy_state_dict': self.policy.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict()\n",
    "            }, path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    env_name = \"LunarLanderContinuous-v2\"\n",
    "    render = False\n",
    "    solved_reward = 200         # stop training if avg_reward > solved_reward\n",
    "    log_interval = 20           # print avg reward in the interval\n",
    "    max_episodes = 10000000      # max training episodes\n",
    "    max_timesteps = 10000         # max timesteps in one episode\n",
    "    n_latent_var = 64           # number of variables in hidden layer\n",
    "    update_timestep = 50000     # update policy every n timesteps\n",
    "    action_std = 0.1#5            # constant std for action distribution\n",
    "    lr = 0.002\n",
    "    betas = (0.9, 0.999)\n",
    "    gamma = 0.99                # discount factor\n",
    "    K_epochs = 5                # update policy for K epochs\n",
    "    eps_clip = 0.2             # clip parameter for PPO\n",
    "    random_seed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node('NN_ROBOT1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Robot = NN_ROBOT1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Robot.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 3\n",
    "action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/giuseppe/catkin_ws/src/self_balancing_ai/notebook/dc_motor_temp.tar'\n",
    "memory = Memory()\n",
    "ppo = PPO(state_dim, action_dim, n_latent_var, action_std, lr, betas, gamma, K_epochs, eps_clip)  \n",
    "rate = rospy.Rate(10) # 10hz\n",
    "ppo.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating policy...\n",
      "Updating policy...\n",
      "Updating policy...\n",
      "Updating policy...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-732ee8206e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mRobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint1_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mRobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoint2_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m#ritardo di movimento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#reward for stay in a vertical position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "running_reward = 0\n",
    "avg_length = 0\n",
    "time_step = 0\n",
    "fine = False\n",
    "best_reward = 0 \n",
    "# training loop\n",
    "best_reward_cnt = 0 \n",
    "old_roll = 0\n",
    "reward_print = 0\n",
    "old_running_reward = 600\n",
    "Robot.reset()\n",
    "Robot.reset_pose()\n",
    "reward_graph = 0\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    Robot.reset()\n",
    "    Robot.reset_pose()\n",
    "    start_time = time.time()\n",
    "    avg_length = 0       \n",
    "    old_pos = 0\n",
    "    old_roll= 0\n",
    "    if running_reward > (best_reward + 5):\n",
    "        best_reward_cnt += 1\n",
    "        best_reward = running_reward\n",
    "        \n",
    "    running_reward = 0\n",
    "    motor_position  = 0\n",
    "    for t in range(max_timesteps):\n",
    "        reward_graph += 1\n",
    "        '''\n",
    "        while(not Robot.new_angle):\n",
    "            continue\n",
    "        Robot.new_angle = False\n",
    "        '''\n",
    "        time_step +=1\n",
    "        Ts = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        vel = (Robot.roll - old_roll)/Ts\n",
    "        old_roll = Robot.roll\n",
    "        state = [Robot.roll, vel, 0]\n",
    "   \n",
    "        # Running policy_old:\n",
    "        action = ppo.select_action(np.array(state), memory)[0] * 10\n",
    "       \n",
    "        Robot.joint1_msg.angular.z = action \n",
    "        Robot.joint2_msg.angular.z = action\n",
    "        Robot.joint2.publish(Robot.joint1_msg)\n",
    "        Robot.joint1.publish(Robot.joint2_msg) \n",
    "        time.sleep(0.02)                  #ritardo di movimento \n",
    "        \n",
    "        reward =  1 - abs(Robot.roll)    #reward for stay in a vertical position\n",
    "            \n",
    "        reward_print += reward\n",
    "        #print (\"Roll: {}\\tReaward: {}\".format(Robot.roll, reward))\n",
    "        if (abs(Robot.roll) > 0.8):\n",
    "            reward = - 100\n",
    "            reward_print += reward\n",
    "            fine = True\n",
    "        # Saving reward:\n",
    "        memory.rewards.append(reward)\n",
    "        \n",
    "            \n",
    "        # update if its time\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo.update(memory)\n",
    "            memory.clear_memory()\n",
    "            time_step = 0\n",
    "        running_reward += reward\n",
    "        if (fine):\n",
    "            fine = False\n",
    "            writer.add_scalar(\"Reward\", running_reward, i_episode)\n",
    "            #print (\"Reward episode: {}\".format(running_reward))\n",
    "            if running_reward > old_running_reward: \n",
    "                ppo.save(PATH) \n",
    "                #print(\"Old reward: {}\".format(old_running_reward))\n",
    "                old_running_reward = running_reward\n",
    "                print(\"Saved\")\n",
    "            reward_print = 0\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/giuseppe/catkin_ws/src/self_balancing_ai/notebook/dc_motor_temp.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.save(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_running_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4541"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 8 \n",
    "for l in range(8,10):\n",
    "    print (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
